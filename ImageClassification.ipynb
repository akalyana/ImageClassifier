{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os \n",
    "import re\n",
    "\n",
    "import keras\n",
    "from scipy import stats\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, GlobalAveragePooling1D, GlobalAveragePooling2D, Flatten, BatchNormalization, Dense\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.constraints import maxnorm\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, SGD , RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "## Load Pretrained image classification models \n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seed Everything \n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses data generators to get data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "\n",
    "def prepare_data(train_path,val_path,test_path,batch_size=256,seed=42): \n",
    "    \n",
    "    ## Initializae values \n",
    "    rescale = 1./255\n",
    "    target_size = (150, 150)\n",
    "    class_mode = \"categorical\"\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=rescale,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train,\n",
    "        target_size=target_size,\n",
    "        class_mode=class_mode,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=False,\n",
    "        seed = seed)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=rescale)\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val,\n",
    "        target_size=target_size,\n",
    "        batch_size = batch_size,\n",
    "        class_mode= class_mode,\n",
    "        seed = seed,\n",
    "        shuffle = False)\n",
    "\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=rescale)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test,\n",
    "        target_size=target_size,\n",
    "        class_mode= class_mode,\n",
    "        seed=seed,\n",
    "        shuffle = False)\n",
    "\n",
    "    return train_generator,val_generator,test_generator\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Keras callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_keras_callbacks(model_file,log_dir,batch_size=256): \n",
    "    checkpoint = ModelCheckpoint(\n",
    "    model_file, \n",
    "    monitor='val_acc', \n",
    "    save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True)\n",
    "\n",
    "\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        batch_size=batch_size,\n",
    "        update_freq = 'batch')\n",
    "\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        cooldown=2,\n",
    "        min_lr=0.0000000001,\n",
    "        verbose=1)\n",
    "\n",
    "    callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n",
    "    \n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "128\n",
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Seed everything so that the outputs are deterministic \n",
    "\n",
    "seed_everything(42)\n",
    "root_dir = os.getcwd() + \"/chest_xray/\"\n",
    "train = root_dir + 'train/'\n",
    "val  = root_dir + 'val/'\n",
    "test  = root_dir + 'test/'\n",
    "out = root_dir + 'output/'\n",
    "\n",
    "# Prepare data generators\n",
    "\n",
    "train_gen, val_gen, test_gen =  prepare_data(train,val,test,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this cell to train inception, vgg and resnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "128\n",
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "41 5 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 263s 6s/step - loss: 0.4424 - acc: 0.8364 - val_loss: 0.7818 - val_acc: 0.7340\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 351s 9s/step - loss: 0.2367 - acc: 0.9012 - val_loss: 1.8212 - val_acc: 0.6795\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 257s 6s/step - loss: 0.2180 - acc: 0.9169 - val_loss: 1.2430 - val_acc: 0.7163\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 259s 6s/step - loss: 0.1927 - acc: 0.9223 - val_loss: 1.5389 - val_acc: 0.7115\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 263s 6s/step - loss: 0.2073 - acc: 0.9151 - val_loss: 0.9943 - val_acc: 0.7147\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 287s 7s/step - loss: 0.1931 - acc: 0.9243 - val_loss: 2.3506 - val_acc: 0.6635\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/10\n",
      "41/41 [==============================] - 1019s 25s/step - loss: 0.3999 - acc: 0.8188 - val_loss: 0.4079 - val_acc: 0.8205\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 1022s 25s/step - loss: 0.2334 - acc: 0.9009 - val_loss: 0.4644 - val_acc: 0.7821\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 1064s 26s/step - loss: 0.2109 - acc: 0.9122 - val_loss: 0.3969 - val_acc: 0.8446\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 900s 22s/step - loss: 0.1890 - acc: 0.9194 - val_loss: 0.3539 - val_acc: 0.8510\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 932s 23s/step - loss: 0.1638 - acc: 0.9339 - val_loss: 0.4078 - val_acc: 0.8526\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 15843s 386s/step - loss: 0.1682 - acc: 0.9350 - val_loss: 0.3498 - val_acc: 0.8638\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 887s 22s/step - loss: 0.1500 - acc: 0.9418 - val_loss: 0.4119 - val_acc: 0.8590\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 1099s 27s/step - loss: 0.1521 - acc: 0.9416 - val_loss: 0.3270 - val_acc: 0.8798\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 982s 24s/step - loss: 0.1511 - acc: 0.9430 - val_loss: 0.2623 - val_acc: 0.8990\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 772s 19s/step - loss: 0.1540 - acc: 0.9414 - val_loss: 0.3371 - val_acc: 0.8734\n",
      "Epoch 1/10\n",
      "41/41 [==============================] - 420s 10s/step - loss: 0.4056 - acc: 0.8494 - val_loss: 1.3812 - val_acc: 0.6314\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 390s 10s/step - loss: 0.1394 - acc: 0.9472 - val_loss: 1.6317 - val_acc: 0.6346\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 390s 10s/step - loss: 0.1165 - acc: 0.9545 - val_loss: 1.8236 - val_acc: 0.6314\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 386s 9s/step - loss: 0.1169 - acc: 0.9545 - val_loss: 2.0491 - val_acc: 0.6330\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 384s 9s/step - loss: 0.1122 - acc: 0.9560 - val_loss: 2.0436 - val_acc: 0.6122\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 386s 9s/step - loss: 0.0956 - acc: 0.9653 - val_loss: 2.3225 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Get base models\n",
    "base_models = [] \n",
    "base_model_type = ['inception','vgg19', 'resnet']\n",
    "mod_models = [] \n",
    "for bt in base_model_type: \n",
    "    base_models.append(get_base_model(model_type=bt,input_shape=(150,150,3),freeze='all'))\n",
    "\n",
    "#Add and retrain dense layers \n",
    "\n",
    "for model in base_models: \n",
    "    mod_models.append(add_last_layer(model))\n",
    "#Setup callbacks \n",
    "\n",
    "model_file = root_dir + 'output/' +  \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "log_dir = root_dir + 'output/log/' \n",
    "callbacks = setup_keras_callbacks(model_file,log_dir,1024)\n",
    "\n",
    "#Train models\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for model in mod_models: \n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch = len(train_gen),\n",
    "        epochs = epochs,\n",
    "        validation_data = val_gen,\n",
    "        validation_steps = len(val_gen),\n",
    "        callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add 2 fully connected layers at the end of Inception, VGG and Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_last_layer(base_model):\n",
    "    x=base_model.output\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "    x=Dropout(.5)(x)\n",
    "    preds=Dense(2,activation='softmax')(x) #final layer w\n",
    "    model = Model(inputs=base_model.input,outputs=preds)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(model_type='inception',input_shape=(150,150,3),freeze='all'): \n",
    "    if(model_type == 'inception'): \n",
    "        base_model = InceptionV3(weights='imagenet',input_shape=(150,150,3), include_top=False)\n",
    "    elif(model_type == 'vgg19'):\n",
    "        base_model = VGG19(weights='imagenet',input_shape=(150,150,3), include_top=False)\n",
    "    elif(model_type == 'resnet'):\n",
    "        base_model = ResNet50(weights='imagenet',input_shape=(150,150,3), include_top=False)\n",
    "    #Decide how much to freeze \n",
    "    if freeze == 'all': \n",
    "        ll = len(base_model.layers)\n",
    "    else: \n",
    "        ll = int(freeze)\n",
    "    for layer in base_model.layers[:ll]:\n",
    "        layer.trainable = False\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict once the models are trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Models and Ensemble to predict\n",
    "##Graph Results \n",
    "def predict(path, data_gen, ensemble=True): \n",
    "    preds = list()\n",
    "    mod_eval = list()\n",
    "    if(ensemble==True): \n",
    "        for f in os.listdir(path):\n",
    "            if f.endswith('.hdf5'):\n",
    "                acc = float(f.split('-')[2])\n",
    "                if(acc>=0.7):\n",
    "                    model = load_model(path+f)\n",
    "                    preds.append(model.predict_generator(data_gen, steps = len(data_gen)))\n",
    "                    mod_eval.append(model.evaluate_generator(data_gen, steps = len(data_gen)))\n",
    "        ###\n",
    "        ne   = preds[0].shape[0]\n",
    "        nc   = preds[0].shape[1] \n",
    "        na   = np.array(preds).reshape(len(preds)*ne,nc)\n",
    "        na   = np.argmax(na, axis=1)\n",
    "        na   = na.reshape(len(preds),ne)\n",
    "        # Voting mechanism for the models \n",
    "        outcomes,_ = stats.mode(na)\n",
    "        outcomes = outcomes.reshape(ne,1)\n",
    "  #              preds.append(model.predict(val_generator, steps= len(val_generator)) \n",
    "        \n",
    "    else: \n",
    "        l_acc = 0 \n",
    "        for f in os.listdir(path): \n",
    "            if f.endswith('.hdf5'):\n",
    "                acc = float(f.split('-')[2]) \n",
    "                if acc > l_acc: \n",
    "                    l_acc = acc\n",
    "                    f_l_acc = f \n",
    "        print('Largest file.{name}'.format(name = f_l_acc))\n",
    "        model = load_model(path+f_l_acc)\n",
    "        preds.append(model.predict_generator(data_gen, steps = len(data_gen)))\n",
    "        mod_eval.append(model.evaluate_generator(data_gen, steps = len(data_gen)))\n",
    "        np_preds = (np.argmax(np.squeeze(np.array(preds),axis=0),axis=1))\n",
    "        outcomes = np_preds \n",
    "\n",
    "            #Parse the highest val accuracy model \n",
    "            #Make the prediction \n",
    "                \n",
    "    return outcomes,preds,mod_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "data_list = []\n",
    "while batch_index <= test_gen.batch_index:\n",
    "    data = test_gen.next()\n",
    "    data_list.append(data)\n",
    "    batch_index = batch_index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with Ensemble and without Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest file.09-val_acc-0.90-val_loss-0.26.hdf5\n"
     ]
    }
   ],
   "source": [
    "outcomes, preds,mod_eval = predict(root_dir + 'output/',test_gen, True)\n",
    "outcomes1, preds1,mod_eval1 = predict(root_dir + 'output/',test_gen, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble result comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Metrics: Precission - 0.7272727272727273 Recall = 1.0 F1 = 0.8421052631578947 Accuracy = 0.8125\n",
      "Without Ensemble Metrics: Precission - 0.7 Recall = 0.875 F1 = 0.7777777777777779 Accuracy = 0.75\n"
     ]
    }
   ],
   "source": [
    "y_actual = np.argmax(data_list[0][1],axis=1)\n",
    "preds = outcomes \n",
    "precision, recall, f1,ac = calc_metrics(y_actual,preds)\n",
    "print(\"Ensemble Metrics: Precission - {Pr} Recall = {Rc} F1 = {f1} Accuracy = {Ac}\".format(Pr=precision,Rc=recall,f1=f1,Ac=ac))\n",
    "preds = outcomes1\n",
    "precision, recall, f1,ac = calc_metrics(y_actual,preds)\n",
    "print(\"Without Ensemble Metrics: Precission - {Pr} Recall = {Rc} F1 = {f1} Accuracy = {Ac}\".format(Pr=precision,Rc=recall,f1=f1,Ac=ac))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Key metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_actual,preds): \n",
    "    recall = np.sum(np.dot(y_actual,preds))/ np.sum(y_actual)\n",
    "    precision = np.sum(np.dot(y_actual,preds))/ np.sum(preds)\n",
    "    f1 = 2 /((1/recall)+(1/precision))\n",
    "    ac  = (np.sum(np.dot(y_actual,preds)) + np.sum(np.dot(1-y_actual,1-preds)))/preds.shape[0]\n",
    "    return precision, recall, f1,ac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(string, pattern):\n",
    "    regex = re.sub(r'{(.+?)}', r'(?P<_\\1>.+)', pattern)\n",
    "    values = list(re.search(regex, string).groups())\n",
    "    keys = re.findall(r'{(.+?)}', pattern)\n",
    "    _dict = dict(zip(keys, values))\n",
    "    return _dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "s = 'p06-val_acc-0.86-val_loss-0.35.hdf5'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
